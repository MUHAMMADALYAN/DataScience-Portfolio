# the S&P500 data is read here and formatted
require(xts)
require(timeSeries)
require(Quandl)
stock=Quandl("CHRIS/CME_SP1", start_date="2011-09-18")
stock=as.xts(as.numeric(stock$Last), order.by=as.Date(stock$Date))
stock=stock[complete.cases(stock)]
weeklystock = apply.weekly(stock, mean)
URL = "http://api.bitcoincharts.com/v1/csv/bitstampUSD.csv.gz"
dir.create("data")
download.file(URL, destfile = "./data/bitcoin.csv")
# This part formats the bitcoin data, converts the Unix time to yyyy-mm-dd format
raw = read.csv("./data/bitcoin.csv", header=FALSE)
names(raw)=c("unixtime", "price", "amount")
raw$date <- as.Date(as.POSIXct(raw$unixtime, origin="1970-01-01"))
# All the transcation is shown when the data is downloaded,
# but we only need the average daily price, here the data is aggregated
raw = aggregate(price~date, data=raw, FUN = mean)
#The Bitcoin price data is changed to a weekly basis here.
data <- as.xts(raw$price,order.by=as.Date(raw$date))
weeklybitcoin <- apply.weekly(data,mean)
plot(weeklystock)
plot(weeklybitcoin)
#the two time series are trim to have the same length
weeklystock = weeklystock[(length(weeklystock)-length(weeklybitcoin)+1):length(weeklystock),]
weeklybitcoin=as.ts(weeklybitcoin)
weeklystock=as.ts(weeklystock)
#compute cross corelation of up to 20 lag
ccf(weeklybitcoin, weeklystock, lag.max = 20)
require(lmtest)
#weekly = weekly[(length(weekly)-length(result$trend$bitcoin.)+1):length(weekly)]
#test for Granger causality
test =  grangertest(weeklybitcoin~weeklystock , order=1)
test2 =  grangertest(weeklystock~weeklybitcoin , order=1)
bitcoints=ts(raw$price, start=c(2011.75), frequency =365.25)
plot.ts(bitcoints)
#HP decomposition of the Bitcoin data. A frequency of 100*365^2 is used
# for daily data
require(mFilter)
bitHP=hpfilter(bitcoints, freq = 100*365^2)
plot(bitHP$trend, ylab='Trend')
plot(bitHP$cycle, ylab='Cycle')
#Differencing the bitcoin data
require(forecast)
bitdif1=diff(bitcoints, differences = 1)
plot.ts(bitdif1, ylab='First_Difference')
#Second Difference the Bitcoin Data
bitdif2=diff(bitcoints, differences = 2)
plot.ts(bitdif2, ylab='Second_Difference')
# Find the ACF and PACF of the Bitcoin data
acf(bitdif1, main='First Difference')
pacf(bitdif1, main='First Difference')
#Cut the bitcion data to remove the bubble part
bitcoincut = window(bitcoints, c(2014.5), end(bitcoints))
plot.ts(bitcoincut)
#Difference the data again
bitcutdif1=diff(bitcoincut, differences = 1)
plot.ts(bitcutdif1, main="First Difference of Cut Series")
#Find ACF and PACF of the difference cut data
acf(bitcutdif1)
pacf(bitcutdif1)
#Find the confidence interval moving forward in Bitcoin price
arorder=auto.arima(bitcoincut, ic="bic")
bitcoinarima=arima(bitcoincut, order=c(2,1,1))
bitcoinforcast=forecast(bitcoinarima, h=180)
plot.forecast(bitcoinforcast)
#plot the second difference of the cut data
bitcutdif2=diff(bitcoincut, differences = 2)
plot.ts(bitcutdif2)
#Find the confidence interval moving forward in Bitcoin price
arorder=auto.arima(bitcoincut, ic="bic")
bitcoinarima=arima(bitcoincut, order=c(2,1,1))
bitcoinforcast=forecast(bitcoinarima, h=180)
plot.forecast(bitcoinforcast)
#Find the confidence interval moving forward in Bitcoin price
arorder=auto.arima(bitcoincut, ic="bic")
bitcoinarima=arima(bitcoincut, order=c(2,1,1))
bitcoinforcast=forecast(bitcoinarima, h=180)
plot(bitcoinforcast)
#predict the price of Bitcoin in 2016 using 2014 - 2015 data
bitcoinoneyear= window(bitcoincut, start(bitcoincut), c(2016))
bitcoinarima=arima(bitcoinoneyear, order=c(2,1,1))
bitcoinforcast=forecast(bitcoinarima, h=270)
# plots both forcast and the actually bitcoin price
plot.forecast(bitcoinforcast)
#predict the price of Bitcoin in 2016 using 2014 - 2015 data
bitcoinoneyear= window(bitcoincut, start(bitcoincut), c(2016))
bitcoinarima=arima(bitcoinoneyear, order=c(2,1,1))
bitcoinforcast=forecast(bitcoinarima, h=270)
# plots both forcast and the actually bitcoin price
plot(bitcoinforcast)
lines(bitcoincut)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
a=5
b=3
c=a+b
a=5
b=3
c=a+b
c
require(qdap)
tweets=read.csv("tweets.csv", header = TRUE, stringsAsFactors = FALSE )
head(tweets)
#create data frame
tweets=data.frame(time=tweets$created_at, text=tweets$text, sentiment=NA, postw=0, negtw=0, tottw=1)
#Detect sentiment for each tweet
for(i in 1:nrow(tweets)){
#Some tweets have no words which would cause error with polarity()
#Therefore the try function
sentiment = try(polarity(sent_detect(tweets$text[i])))
if(class(sentiment)=="try-error"){
tweets$sentiment[i]=NA
}else{
#sentiment is assigned value returned from polarity() function
tweets$sentiment[i]=sentiment$group$ave.polarity
if(is.nan(sentiment$group$ave.polarity)){
next
#0.3 is also used as classification threshold for future applications
}else if(sentiment$group$ave.polarity>0.3){
tweets$postw[i]=1
}else if(sentiment$group$ave.polarity<(-0.3)){
tweets$negtw[i]=1
}
}
}
#remove tweets that qdap() cannot distinguish
tweets=tweets[!is.nan(tweets$sentiment)&!is.na(tweets$sentiment),]
head(tweets)
head(tweets)
head(tweets$text, tweets, tweets$postw, tweets$negtw)
head(tweets$text)
head(cbind(tweets$text, tweets, tweets$postw, tweets$negtw))
head(cbind(tweets$text, tweets$postw, tweets$negtw))
tweets$text
head(cbind(tweets$text, tweets$postw, tweets$negtw))
cbind(tweets$text, tweets$postw, tweets$negtw)
head(tweets['text', 'postw', 'negtw'])
head(tweets[['text', 'postw', 'negtw']])
tweets['text']
tweets['text', 'postw']
head(tweets[,c('text', 'postw', 'negtw')])
URL = "https://www.ssa.gov/oact/babynames/names.zip"
dir.create("data")
download.file(URL, destfile = "./data/babyname.zip")
unzip("./data/babyname.zip", exdir = "./data")
table=data.frame()
for( i in 1960:2014){
filename= paste("./data/yob", i, ".txt", sep = "")
table = rbind(table ,read.csv(file=filename, header=FALSE, sep = ',', stringsAsFactors = FALSE))
}
colnames(table) = c("name", "gender", "count")
table = aggregate(count~name, data=table, FUN=sum)
table = table[order(table$count, decreasing = TRUE),]
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE)
plot(r$mids, r$counts, log="xy")
CCDF=vector(mode="numeric", length=nrow(table))
CCDF[length(CCDF)]=table$count[length(CCDF)]
for(i in 2:length(CCDF)){
CCDF[length(CCDF)-i+1] = CCDF[length(CCDF)-i+2]+table$count[length(CCDF)-i+1]
}
plot(CCDF, log = "xy")
require(poweRlaw)
mm=displ$new(table$count)
install.packages('poweRlaw')
require(poweRlaw)
mm=displ$new(table$count)
mm$getXmin()
mm$getPars()
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
bs=bootstrap(mm, no_of_sims = 200, threads = 2)
require(poweRlaw)
mm=displ$new(table$count)
mm$getXmin()
mm$getPars()
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
require(twitteR)
consumer_key <- 'X2QkLTfDYZJQee3eZIfs4PbfO'
consumer_secret <- 'HKeDcjgqlRkhXGg4W0FBNa46xN05IRERWHCnvcI3hYhOKHgexh'
access_token <- '747703282408296448-J5EVuVKvBkz7fG22LsOURiZPG8IefB3'
access_secret <- 'TClX1U2Tscj0QCCL5fGVWwhjjx4m0vJjtuFrlqvHcwkLI'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
install.packages('twitteR')
require(twitteR)
consumer_key <- 'X2QkLTfDYZJQee3eZIfs4PbfO'
consumer_secret <- 'HKeDcjgqlRkhXGg4W0FBNa46xN05IRERWHCnvcI3hYhOKHgexh'
access_token <- '747703282408296448-J5EVuVKvBkz7fG22LsOURiZPG8IefB3'
access_secret <- 'TClX1U2Tscj0QCCL5fGVWwhjjx4m0vJjtuFrlqvHcwkLI'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
users=list()
while(length(users)<=10000){
rate=getCurRateLimitInfo()
while(as.numeric(rate$remaining[rate$resource=="/users/lookup"])<20){
Sys.sleep(100)
rate=getCurRateLimitInfo()
if(as.numeric(rate$remaining[rate$resource=="/users/lookup"])>=20){
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
}
}
a=ceiling(runif(2000, min=0, max=3E9))
users=unique(c(lookupUsers(a),users))
}
table=data.frame(matrix(0, ncol=5, nrow=length(users)))
colnames(table)=c("users", "status_count", "follower_count", "favorites_count", "friends_count")
i=1
for (person in users){
table$users[i] = person$screenName
table$status_count[i] = sort(person$statusesCount)
table$follower_count[i] = person$followersCount
table$favorites_count[i] = person$favoritesCount
table$friends_count[i] = person$friendsCount
i=i+1
}
require(poweRlaw)
mm=displ$new(table$status_count[table$status_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(table$follower_count[table$follower_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(table$favorites_count[table$favoristes_count!=0])
mm=displ$new(table$favorites_count[table$favoristes_count!=0])
table
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE, breaks=50)
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE, breaks=20)
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE, breaks=10)
countprob = table$count/sum(table$count)
r <- hist(countprob, breaks=10, plot=FALSE)
table=data.frame()
for( i in 1960:2014){
filename= paste("./data/yob", i, ".txt", sep = "")
table = rbind(table ,read.csv(file=filename, header=FALSE, sep = ',', stringsAsFactors = FALSE))
}
colnames(table) = c("name", "gender", "count")
table = aggregate(count~name, data=table, FUN=sum)
table = table[order(table$count, decreasing = TRUE),]
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE,breaks=50)
plot(r$mids, r$counts, log="xy")
CCDF=vector(mode="numeric", length=nrow(table))
CCDF[length(CCDF)]=table$count[length(CCDF)]
for(i in 2:length(CCDF)){
CCDF[length(CCDF)-i+1] = CCDF[length(CCDF)-i+2]+table$count[length(CCDF)-i+1]
}
plot(CCDF, log = "xy", title="Baby Name CCDF")
plot(CCDF, log = "xy", main="Baby Name CCDF")
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE,breaks=50)
plot(r$mids, r$counts, log="xy", main = "Baby name Histogram")
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE,breaks=50)
plot(r$mids, r$counts, log="xy", main = "Baby name Histogram")
twtable=data.frame(matrix(0, ncol=5, nrow=length(users)))
colnames(twtable)=c("users", "status_count", "follower_count", "favorites_count", "friends_count")
i=1
for (person in users){
twtable$users[i] = person$screenName
twtable$status_count[i] = sort(person$statusesCount)
twtable$follower_count[i] = person$followersCount
twtable$favorites_count[i] = person$favoritesCount
twtable$friends_count[i] = person$friendsCount
i=i+1
}
require(poweRlaw)
mm=displ$new(twtable$status_count[twtable$status_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(twtable$follower_count[twtable$follower_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(twtable$favorites_count[twtable$favoristes_count!=0])
require(poweRlaw)
mm=displ$new(twtable$status_count[twtable$status_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(twtable$follower_count[twtable$follower_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(twtable$favorites_count[twtable$favorites_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(twtable$friends_count[twtable$friends_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
X = table$babycount
CCDF = ecdf(X)
table$babycount
countprob = table$count/sum(table$count)
r <- hist(countprob, plot=FALSE,breaks=50)
plot(r$mids, r$counts, log="xy", main = "Baby name Histogram")
table
X = table$babycount
CCDF = ecdf(X)
table
X = table$count
CCDF = ecdf(X)
xval = sort(unique(CCDF))
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CDFfun(xs)
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CDFfun(xval)
install.packages('migrittr')
library(magrittr)
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CDFfun(xval)
library(dplyr)
table=data.frame()
for( i in 1960:2014){
filename= paste("./data/yob", i, ".txt", sep = "")
table = rbind(table ,read.csv(file=filename, header=FALSE, sep = ',', stringsAsFactors = FALSE))
}
colnames(table) = c("name", "gender", "count")
table = aggregate(count~name, data=table, FUN=sum)
table = table[order(table$count, decreasing = TRUE),]
library(magrittr)
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CDFfun(xval)
xval
library(magrittr)
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CDFfun(xval)
library(magrittr)
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CCDF(xval)
plot(xval, 1-yval,log = "xy", main="Baby Name CCDF", ylab="P(X>x")
library(magrittr)
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CCDF(xval)
plot(xval, 1-yval,log = "xy", main="Baby Name CCDF", ylab="P(X>x)")
library(magrittr)
X = table$count
CCDF = ecdf(X)
xval = sort(unique(X))
yval = CCDF(xval)
plot(xval, 1-yval,log = "xy", main="Baby Name CCDF", ylab="P(X>x)", xlab='babies')
count = table$count
r <- hist(count, plot=FALSE,breaks=50)
plot(r$mids, r$counts, log="xy", main = "Baby name Histogram", ylab ='frequency', xlab='babies')
require(poweRlaw)
mm=displ$new(table$count)
mm$getXmin()
mm$getPars()
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
legend("bottomleft", "Power-law", col="red")
mm2 = dislnorm$new(X)
#mm2$setXmin(30145) #If you want to analyze the top 1000
est2 = estimate_pars(mm2)
mm2$setPars(est2)
plot(mm,xlab="Baby name sample",ylab="Baby name counts", main="")
lines(mm, col=2)
lines(mm2, col=3)
legend("bottomleft", c("power-law","log-normal"), col=c(2,3), lwd=c(1,1))
URL = "https://www.ssa.gov/oact/babynames/names.zip"
dir.create("data")
download.file(URL, destfile = "./data/babyname.zip")
unzip("./data/babyname.zip", exdir = "./data")
library(dplyr)
table=data.frame()
for( i in 1960:2015){
filename= paste("./data/yob", i, ".txt", sep = "")
table = rbind(table ,read.csv(file=filename, header=FALSE, sep = ',', stringsAsFactors = FALSE))
}
colnames(table) = c("name", "gender", "count")
table = aggregate(count~name, data=table, FUN=sum)
table = table[order(table$count, decreasing = TRUE),]
head(table)
require(twitteR)
consumer_key <- 'X2QkLTfDYZJQee3eZIfs4PbfO'
consumer_secret <- 'HKeDcjgqlRkhXGg4W0FBNa46xN05IRERWHCnvcI3hYhOKHgexh'
access_token <- '747703282408296448-J5EVuVKvBkz7fG22LsOURiZPG8IefB3'
access_secret <- 'TClX1U2Tscj0QCCL5fGVWwhjjx4m0vJjtuFrlqvHcwkLI'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
users=list()
while(length(users)<=20000){
rate=getCurRateLimitInfo()
while(as.numeric(rate$remaining[rate$resource=="/users/lookup"])<20){
#the sleep is for waiting to reset the Twitter quota.
Sys.sleep(100)
rate=getCurRateLimitInfo()
if(as.numeric(rate$remaining[rate$resource=="/users/lookup"])>=20){
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
}
}
a=ceiling(runif(2000, min=0, max=3E9))
users=unique(c(lookupUsers(a),users))
}
twtable=data.frame(matrix(0, ncol=5, nrow=length(users)))
colnames(twtable)=c("users", "status_count", "follower_count", "favorites_count", "friends_count")
i=1
for (person in users){
twtable$users[i] = person$screenName
twtable$status_count[i] = sort(person$statusesCount)
twtable$follower_count[i] = person$followersCount
twtable$favorites_count[i] = person$favoritesCount
twtable$friends_count[i] = person$friendsCount
i=i+1
}
i=1
for (person in users){
twtable$users[i] = person$screenName
twtable$status_count[i] = sort(person$statusesCount)
twtable$follower_count[i] = person$followersCount
twtable$favorites_count[i] = person$favoritesCount
twtable$friends_count[i] = person$friendsCount
i=i+1
}
require(poweRlaw)
mm=displ$new(twtable$follower_count[twtable$follower_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
mm=displ$new(twtable$friends_count[twtable$friends_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm)
lines(mm, col=2)
require(poweRlaw)
mm=displ$new(twtable$follower_count[twtable$follower_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm, main='Twitter Followers Count Distribution', ylab='P(X>x)', xlab='Followers Count')
lines(mm, col=2)
mm=displ$new(twtable$friends_count[twtable$friends_count!=0])
(est=estimate_pars(mm))
(est=estimate_xmin(mm))
mm$setXmin(est)
plot(mm, main='Twitter Friends Count Distribution', ylab='P(X>x)', xlab='Friends Count')
lines(mm, col=2)
bs=bootstrap(mm, no_of_sims = 100, threads = 2)
